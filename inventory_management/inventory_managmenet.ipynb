{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inventory Management with Reinforcement Learning\n",
    "\n",
    "This Jupyter notebook explores the application of Reinforcement Learning (RL) techniques in solving an inventory management problem. The goal is to train an agent to make optimal decisions regarding the quantity of products to order in a dynamic environment, balancing costs associated with overstocking and potential lost sales due to understocking.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "\n",
    "1. **Environment Definition**\n",
    "   - We start by defining a custom environment named `InventoryEnv` using the Gym library. This environment models the inventory management problem and provides a platform for the RL agent to interact with.\n",
    "\n",
    "2. **Benchmark Policy**\n",
    "   - We introduce a benchmark policy function that serves as a baseline strategy for comparison. This policy computes the order quantity based on a cost analysis of overage and underage.\n",
    "\n",
    "3. **Reinforcement Learning Training**\n",
    "   - We utilize the Proximal Policy Optimization (PPO) algorithm from Ray's RLlib library to train an agent. This involves setting up the PPO configuration, initializing the environment and trainer, and running the training loop.\n",
    "\n",
    "4. **Loading and Testing Trained Model**\n",
    "   - Once the agent is trained, we demonstrate how to load the trained model and evaluate its performance in the `InventoryEnv` environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This Python code defines a custom environment called `InventoryEnv` using the Gym library. This environment models a simplified inventory management problem.\n",
    "\n",
    "## Environment Details\n",
    "1. **Initialization**:\n",
    "    - The environment is initialized with several parameters, such as `lead time`, `storage_capacity`, `order_limit`, and others.\n",
    "    - There are also maximum values defined for various quantities like `max_value`, `max_holding_cost`, etc.\n",
    "\n",
    "2. **Observation Space**:\n",
    "    - The state space is defined as a high-dimensional continuous space using the `spaces.Box` class from Gym.\n",
    "    - It includes both inventory levels and various properties associated with the environment.\n",
    "\n",
    "3. **Action Space**:\n",
    "    - The action space is a one-dimensional continuous space, representing the quantity to be ordered. It ranges from 0 to 1, which will be mapped to an order quantity from 0 up to the order limit.\n",
    "\n",
    "4. **State**:\n",
    "    - The state of the environment is represented as an array, which includes information about inventory levels, prices, costs, and other parameters.\n",
    "\n",
    "5. **Methods**:\n",
    "    - `_normalize_obs`: This method normalizes the observations to a specific range, ensuring consistency in the state representation.\n",
    "\n",
    "    - `reset`: Resets the environment to its initial state, including generating random values for parameters like price, cost, holding cost, etc.\n",
    "\n",
    "    - `break_state`: Extracts specific components from the state for better readability and understanding.\n",
    "\n",
    "    - `step`: Simulates a step in the environment. It takes an action (order quantity) as input and computes the resulting state, reward, and whether the episode is done.\n",
    "\n",
    "## Step Function Details\n",
    "1. **Order Placement**:\n",
    "    - The action received is clipped to be between 0 and 1 and then mapped to an order quantity within the order limit.\n",
    "\n",
    "2. **Inventory Update**:\n",
    "    - The available capacity for inventory is calculated, and the actual quantity bought is determined based on this capacity.\n",
    "\n",
    "3. **Demand Realization**:\n",
    "    - The demand for the day is drawn from a Poisson distribution with mean `mu`.\n",
    "\n",
    "4. **Reward Computation**:\n",
    "    - The reward is computed based on various factors, including sales revenue, purchase cost, holding costs, and penalties for lost sales.\n",
    "\n",
    "5. **State Update**:\n",
    "    - The inventory levels are updated for the next day, and in-transit inventory is accounted for.\n",
    "\n",
    "6. **Termination**:\n",
    "    - If the maximum number of steps is reached, the episode is marked as done.\n",
    "\n",
    "7. **Normalization of Reward**:\n",
    "    - The reward is scaled down for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is modified from:\n",
    "https://github.com/awslabs/or-rl-benchmarks/blob/master/News%20Vendor/src/news_vendor_environment.py\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "class InventoryEnv(gym.Env):\n",
    "    def __init__(self, config={}):\n",
    "        self.l = config.get(\"lead time\", 5)\n",
    "        self.storage_capacity = 4000\n",
    "        self.order_limit = 1000\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 40\n",
    "\n",
    "        self.max_value = 100.0\n",
    "        self.max_holding_cost = 5.0\n",
    "        self.max_loss_goodwill = 10.0\n",
    "        self.max_mean = 200\n",
    "\n",
    "        self.inv_dim = max(1, self.l)\n",
    "        space_low = self.inv_dim * [0]\n",
    "        space_high = self.inv_dim * [self.storage_capacity]\n",
    "        space_low += 5 * [0]\n",
    "        space_high += [\n",
    "            self.max_value,\n",
    "            self.max_value,\n",
    "            self.max_holding_cost,\n",
    "            self.max_loss_goodwill,\n",
    "            self.max_mean,\n",
    "        ]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array(space_low),\n",
    "            high=np.array(space_high),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Action is between 0 and 1, representing order quantity from\n",
    "        # 0 up to the order limit.\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0]),\n",
    "            high=np.array([1]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def _normalize_obs(self):\n",
    "        obs = np.array(self.state)\n",
    "        obs[:self.inv_dim] = obs[:self.inv_dim] / self.order_limit\n",
    "        obs[self.inv_dim] = obs[self.inv_dim] / self.max_value\n",
    "        obs[self.inv_dim + 1] = obs[self.inv_dim + 1] / self.max_value\n",
    "        obs[self.inv_dim + 2] = obs[self.inv_dim + 2] / self.max_holding_cost\n",
    "        obs[self.inv_dim + 3] = obs[self.inv_dim + 3] / self.max_loss_goodwill\n",
    "        obs[self.inv_dim + 4] = obs[self.inv_dim + 4] / self.max_mean\n",
    "        return obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_count = 0\n",
    "\n",
    "        price = np.random.rand() * self.max_value\n",
    "        cost = np.random.rand() * price\n",
    "        holding_cost = np.random.rand() * min(cost, self.max_holding_cost)\n",
    "        loss_goodwill = np.random.rand() * self.max_loss_goodwill\n",
    "        mean_demand = np.random.rand() * self.max_mean\n",
    "\n",
    "        self.state = np.zeros(self.inv_dim + 5)\n",
    "        self.state[self.inv_dim] = price\n",
    "        self.state[self.inv_dim + 1] = cost\n",
    "        self.state[self.inv_dim + 2] = holding_cost\n",
    "        self.state[self.inv_dim + 3] = loss_goodwill\n",
    "        self.state[self.inv_dim + 4] = mean_demand\n",
    "\n",
    "        return self._normalize_obs()\n",
    "\n",
    "    def break_state(self):\n",
    "        inv_state = self.state[: self.inv_dim]\n",
    "        p = self.state[self.inv_dim]\n",
    "        c = self.state[self.inv_dim + 1]\n",
    "        h = self.state[self.inv_dim + 2]\n",
    "        k = self.state[self.inv_dim + 3]\n",
    "        mu = self.state[self.inv_dim + 4]\n",
    "        return inv_state, p, c, h, k, mu\n",
    "\n",
    "    def step(self, action):\n",
    "        beginning_inv_state, p, c, h, k, mu = \\\n",
    "            self.break_state()\n",
    "        action = np.clip(action[0], 0, 1)\n",
    "        action = int(action * self.order_limit)\n",
    "        done = False\n",
    "\n",
    "        available_capacity = self.storage_capacity \\\n",
    "                             - np.sum(beginning_inv_state)\n",
    "        assert available_capacity >= 0\n",
    "        buys = min(action, available_capacity)\n",
    "        # If lead time is zero, immediately\n",
    "        # increase the inventory\n",
    "        if self.l == 0:\n",
    "            self.state[0] += buys\n",
    "        on_hand = self.state[0]\n",
    "        demand_realization = np.random.poisson(mu)\n",
    "\n",
    "        # Compute Reward\n",
    "        sales = min(on_hand,\n",
    "                    demand_realization)\n",
    "        sales_revenue = p * sales\n",
    "        overage = on_hand - sales\n",
    "        underage = max(0, demand_realization\n",
    "                          - on_hand)\n",
    "        purchase_cost = c * buys\n",
    "        holding = overage * h\n",
    "        penalty_lost_sale = k * underage\n",
    "        reward = sales_revenue \\\n",
    "                 - purchase_cost \\\n",
    "                 - holding \\\n",
    "                 - penalty_lost_sale\n",
    "\n",
    "        # Day is over. Update the inventory\n",
    "        # levels for the beginning of the next day\n",
    "        # In-transit inventory levels shift to left\n",
    "        self.state[0] = 0\n",
    "        if self.inv_dim > 1:\n",
    "            self.state[: self.inv_dim - 1] \\\n",
    "                = self.state[1: self.inv_dim]\n",
    "        self.state[0] += overage\n",
    "        # Add the recently bought inventory\n",
    "        # if the lead time is positive\n",
    "        if self.l > 0:\n",
    "            self.state[self.l - 1] = buys\n",
    "        self.step_count += 1\n",
    "        if self.step_count >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        # Normalize the reward\n",
    "        reward = reward / 10000\n",
    "        info = {\n",
    "            \"demand realization\": demand_realization,\n",
    "            \"sales\": sales,\n",
    "            \"underage\": underage,\n",
    "            \"overage\": overage,\n",
    "        }\n",
    "        return self._normalize_obs(), reward, done, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Policy Function\n",
    "\n",
    "#### `get_action_from_benchmark_policy(env)`\n",
    "This function computes an action based on a simple benchmark policy. Here's detail for its steps:\n",
    "\n",
    "1. **State Extraction**:\n",
    "   - It first extracts specific components from the environment state using the `break_state` method. This includes inventory state, product price, product cost, holding cost, loss goodwill cost, and mean demand.\n",
    "\n",
    "2. **Cost Computation**:\n",
    "   - It calculates the cost of overage and the cost of underage based on the environment parameters `h` (holding cost), `p` (product price), `c` (product cost), and `k` (loss goodwill cost).\n",
    "\n",
    "3. **Critical Ratio**:\n",
    "   - The critical ratio is calculated. This ratio is used to determine the order quantity in order to balance the costs of overage and underage. It is the ratio of the cost of underage to the total cost (overage + underage).\n",
    "\n",
    "4. **Horizon Target**:\n",
    "   - The desired inventory level (`horizon_target`) is computed using the critical ratio and the mean demand `mu` for the given state.\n",
    "\n",
    "5. **Deficit Calculation**:\n",
    "   - The deficit is calculated as the difference between the desired inventory level and the current inventory level.\n",
    "\n",
    "6. **Buy Action**:\n",
    "   - The buy action is set to be the smaller of the deficit and the order limit defined in the environment.\n",
    "\n",
    "7. **Action Normalization**:\n",
    "   - The buy action is normalized to the range [0, 1] and returned as a list.\n",
    "\n",
    "### Main Section\n",
    "\n",
    "#### `if __name__ == \"__main__\":`\n",
    "This block checks if the script is being run directly (not imported as a module).\n",
    "\n",
    "#### Environment Initialization\n",
    "\n",
    "- A random seed is set for reproducibility.\n",
    "- An instance of `InventoryEnv` is created.\n",
    "\n",
    "#### Episode Loop\n",
    "\n",
    "The following actions are taken for each of the 2000 episodes:\n",
    "\n",
    "1. **Episode Initialization**:\n",
    "   - The episode counter is printed.\n",
    "   - The environment is reset to its initial state, and the initial state is obtained.\n",
    "\n",
    "2. **Episode Execution Loop**:\n",
    "   - A loop runs until the episode is done.\n",
    "   - In each iteration, the benchmark policy function is called to get an action.\n",
    "   - The action is used to interact with the environment using `env.step(action)`.\n",
    "\n",
    "   - The state, reward, and termination status are obtained.\n",
    "   - The reward is appended to a list of episode rewards.\n",
    "\n",
    "3. **Episode Summary**:\n",
    "   - The total reward for the episode is computed as the sum of the rewards.\n",
    "   - The average reward per time step is calculated.\n",
    "\n",
    "4. **Printing Episode Statistics**:\n",
    "   - Information about the current episode's rewards is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_from_benchmark_policy(env):\n",
    "    inv_state, p, c, h, k, mu = env.break_state()\n",
    "    cost_of_overage = h\n",
    "    cost_of_underage = p - c + k\n",
    "    critical_ratio = np.clip(\n",
    "        0, 1, cost_of_underage\n",
    "              / (cost_of_underage + cost_of_overage)\n",
    "    )\n",
    "    horizon_target = int(poisson.ppf(critical_ratio,\n",
    "                         (len(inv_state) + 1) * mu))\n",
    "    deficit = max(0, horizon_target - np.sum(inv_state))\n",
    "    buy_action = min(deficit, env.order_limit)\n",
    "    return [buy_action / env.order_limit]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(100)\n",
    "    env = InventoryEnv()\n",
    "    episode_reward_avgs = []\n",
    "    episode_total_rewards = []\n",
    "    for i in range(2000):\n",
    "        print(f\"Episode: {i+1}\")\n",
    "        initial_state = env.reset()\n",
    "        done = False\n",
    "        ep_rewards = []\n",
    "        while not done:\n",
    "            # action = env.action_space.sample()\n",
    "            action = get_action_from_benchmark_policy(env)\n",
    "            # print(\"Action: \", action)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            # print(\"State: \", state)\n",
    "            ep_rewards.append(reward)\n",
    "        total_reward = np.sum(ep_rewards)\n",
    "        reward_per_day = np.mean(ep_rewards)\n",
    "        # print(f\"Total reward: {total_reward}\")\n",
    "        # print(f\"Reward per time step: {reward_per_day}\")\n",
    "        episode_reward_avgs.append(reward_per_day)\n",
    "        episode_total_rewards.append(total_reward)\n",
    "        print(\n",
    "            f\"Average daily reward over {len(episode_reward_avgs)} \"\n",
    "            f\"test episodes: {np.mean(episode_reward_avgs)}. \"\n",
    "            f\"Average total epsisode reward: {np.mean(episode_total_rewards)}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray and PPO Configuration\n",
    "\n",
    "#### Import Statements\n",
    "- The necessary libraries are imported, including NumPy for numerical operations, Ray for distributed computing, and the PPOTrainer from Ray's RLlib library.\n",
    "\n",
    "#### Configuration\n",
    "- A configuration dictionary (`config`) for PPO training is defined, which will be used to customize the training process.\n",
    "\n",
    "- `num_gpus`: Specifies the number of GPUs to be used. In this case, it's set to 1, assuming a single GPU is available. Set to 0 if no GPU is available.\n",
    "\n",
    "- `num_workers`: Determines the number of parallel workers used for collecting samples. This can be adjusted based on the available CPU cores.\n",
    "\n",
    "- Several hyperparameters are set, influencing the learning process (e.g., learning rate, batch sizes, etc.). Two combinations (commented as \"Combination 1\" and \"Combination 2\") are provided, and one of them is chosen for training.\n",
    "\n",
    "### Ray Initialization\n",
    "\n",
    "- `ray.init()`: Initializes the Ray library, setting up the distributed computing environment for training.\n",
    "\n",
    "### PPO Trainer Initialization\n",
    "\n",
    "- `trainer = PPOTrainer(config=config, env=InventoryEnv)`: Initializes the PPO trainer using the provided configuration and the `InventoryEnv` environment.\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "- The training loop is initiated with a `while True:` loop, indicating that training will continue indefinitely until manually interrupted.\n",
    "\n",
    "- `result = trainer.train()`: This line performs a single training iteration using PPO. The resulting object `result` contains various metrics and information about the training process.\n",
    "\n",
    "- `print(pretty_print(result))`: This prints the training results in a readable format.\n",
    "\n",
    "- `mean_reward = result.get(\"episode_reward_mean\", np.NINF)`: Extracts the mean episode reward from the training result.\n",
    "\n",
    "- If the current mean reward is greater than the best mean reward seen so far (`best_mean_reward`), the model checkpoint is saved, and `best_mean_reward` is updated.\n",
    "\n",
    "- The training loop continues, with the model improving over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "\n",
    "from inventory_env import InventoryEnv\n",
    "\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     num_gpus = 1\n",
    "#     print(f\"Number GPU {num_gpus}\")\n",
    "# else:\n",
    "#     num_gpus = 0\n",
    "#     print(f\"Number GPU {num_gpus}\")\n",
    "\n",
    "\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "config[\"env\"] = InventoryEnv\n",
    "config[\"num_gpus\"] = 1  # Set this to 0 if you don't have a GPU.\n",
    "config[\"num_workers\"] = 4  # Set this based on the number of CPUs on your machine\n",
    "# config[\"num_iterations\"] = 5\n",
    "# Combination 1\n",
    "# config[\"clip_param\"] = 0.3\n",
    "# config[\"entropy_coeff\"] = 0\n",
    "# config[\"grad_clip\"] = 0.01\n",
    "# config[\"kl_target\"] = 0.05\n",
    "# config[\"lr\"] = 0.0001\n",
    "# config[\"num_sgd_iter\"] = 10\n",
    "# config[\"sgd_minibatch_size\"] = 128\n",
    "# config[\"train_batch_size\"] = 10000\n",
    "# config[\"use_gae\"] = True\n",
    "# config[\"vf_clip_param\"] = 10\n",
    "# config[\"vf_loss_coeff\"] = 1\n",
    "# config[\"vf_share_layers\"] = True\n",
    "\n",
    "# Combination 2\n",
    "config[\"clip_param\"] = 0.3\n",
    "config[\"entropy_coeff\"] = 0\n",
    "config[\"grad_clip\"] = None\n",
    "config[\"kl_target\"] = 0.005\n",
    "config[\"lr\"] = 0.001\n",
    "config[\"num_sgd_iter\"] = 5\n",
    "config[\"sgd_minibatch_size\"] = 8192\n",
    "config[\"train_batch_size\"] = 20000\n",
    "config[\"use_gae\"] = True\n",
    "config[\"vf_clip_param\"] = 10\n",
    "config[\"vf_loss_coeff\"] = 1\n",
    "config[\"vf_share_layers\"] = False\n",
    "\n",
    "# For better gradient estimates in the later stages\n",
    "# of the training, increase the batch sizes.\n",
    "# config[\"sgd_minibatch_size\"] = 8192 * 4\n",
    "# config[\"train_batch_size\"] = 20000 * 10\n",
    "\n",
    "ray.init()\n",
    "trainer = PPOTrainer(config=config, env=InventoryEnv)\n",
    "\n",
    "# Use this when you want to continue from a checkpoint.\n",
    "# trainer.restore(\n",
    "#   \"/home/enes/ray_results/PPO_InventoryEnv_2020-10-06_04-31-2945lwn1wg/checkpoint_737/checkpoint-737\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "best_mean_reward = np.NINF\n",
    "while True:\n",
    "    result = trainer.train()\n",
    "    print(pretty_print(result))\n",
    "    mean_reward = result.get(\"episode_reward_mean\", np.NINF)\n",
    "    if mean_reward > best_mean_reward:\n",
    "        checkpoint = trainer.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)\n",
    "        best_mean_reward = mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Testing a Trained Model\n",
    "\n",
    "The provided code is for loading a pre-trained model and testing it in the `InventoryEnv` environment. Let's break down what it does:\n",
    "\n",
    "### Ray and PPO Configuration\n",
    "\n",
    "- Similar to the previous code snippet, it starts with importing necessary libraries and setting up the PPO trainer's configuration using default settings. The environment is set to `InventoryEnv`.\n",
    "\n",
    "### Ray Initialization\n",
    "\n",
    "- `ray.init()`: Initializes Ray for distributed computing.\n",
    "\n",
    "### Restoring a Checkpoint\n",
    "\n",
    "- `trainer.restore()`: Restores a pre-trained model. You need to replace the path inside the parentheses with the actual checkpoint path. This is how you load a previously trained model to either continue training or to perform evaluation/testing.\n",
    "\n",
    "### Main Section\n",
    "\n",
    "- `if __name__ == \"__main__\":` checks if the script is being run directly.\n",
    "\n",
    "- `np.random.seed(0)`: Sets a seed for reproducibility.\n",
    "\n",
    "- `env = InventoryEnv()`: Creates an instance of the `InventoryEnv`.\n",
    "\n",
    "- `episode_reward_avgs` and `episode_total_rewards` are empty lists that will be used to store rewards during testing.\n",
    "\n",
    "### Testing Loop\n",
    "\n",
    "- A loop is initiated for running a series of episodes (2000 in this case).\n",
    "\n",
    "- For each episode:\n",
    "\n",
    "    - `state = env.reset()`: Resets the environment and gets the initial state.\n",
    "    \n",
    "    - A loop is initiated to run a single episode until termination:\n",
    "    \n",
    "        - `action = trainer.compute_action(state)`: The trained policy network computes an action based on the current state.\n",
    "        \n",
    "        - `state, reward, done, info = env.step(action)`: The action is applied to the environment, and the resulting state, reward, and termination flag are obtained.\n",
    "        \n",
    "        - The reward is appended to `ep_rewards`.\n",
    "        \n",
    "    - Total rewards and average rewards per time step for the episode are computed and printed.\n",
    "\n",
    "    - The average daily reward and average total episode reward over all episodes up to this point are calculated and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "from ray.rllib.agents.ppo.ppo import PPOTrainer\n",
    "\n",
    "from inventory_env import InventoryEnv\n",
    "\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "config[\"env\"] = InventoryEnv\n",
    "\n",
    "ray.init()\n",
    "trainer = PPOTrainer(config=config, env=InventoryEnv)\n",
    "\n",
    "trainer.restore(\n",
    "    # Replace this with your checkpoint path.\n",
    "    \"ray_results/PPO_InventoryEnv_2023-09-26_14-57-438z9flexj/checkpoint_3300/checkpoint-3300\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    env = InventoryEnv()\n",
    "    episode_reward_avgs = []\n",
    "    episode_total_rewards = []\n",
    "    for i in range(2000):\n",
    "        print(f\"Episode: {i+1}\")\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_rewards = []\n",
    "        while not done:\n",
    "            action = trainer.compute_action(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ep_rewards.append(reward)\n",
    "        total_reward = np.sum(ep_rewards)\n",
    "        reward_per_day = np.mean(ep_rewards)\n",
    "        print(f\"Total reward: {total_reward}\")\n",
    "        print(f\"Reward per time step: {reward_per_day}\")\n",
    "        episode_reward_avgs.append(reward_per_day)\n",
    "        episode_total_rewards.append(total_reward)\n",
    "        print(\n",
    "            f\"Average daily reward over {len(episode_reward_avgs)} \"\n",
    "            f\"test episodes: {np.mean(episode_reward_avgs)}. \"\n",
    "            f\"Average total epsisode reward: {np.mean(episode_total_rewards)}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
