{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Bandit (Slot Machine) Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, first we'll create a Python class called `GaussianBandi` to simulate a set of bandit machines, also known as slot machines, which provide rewards following a Gaussian (Normal) distribution. The GaussianBandit class will allow us to model the uncertainty and randomness associated with each machine's reward distribution. Each instance of the `GaussianBandit` class represents a different bandit machine. By creating multiple instances with different mean and standard deviation values, we can simulate a set of bandit machines, each having its own reward distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-armed Bandit Problem\n",
    "This code allows us to model the environment of the \"Multi-Armed Bandit\" problem. In this classic problem, an agent (player) is faced with multiple bandit machines and must decide which machine to pull in each round to maximize their total reward over time.\n",
    "\n",
    "The challenge arises from the uncertainty in each machine's reward distribution. As rewards are generated stochastically, the agent cannot know the true mean reward of each machine initially. The agent's objective is to learn and adapt its strategy over time to make better decisions, balancing exploration (trying different machines) and exploitation (choosing machines that appear to have high rewards based on current information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Description\n",
    "\n",
    "#### Constructor:\n",
    "- The `GaussianBandit` class has a constructor `__init__(self, mean=0, stdev=1)`, which takes two optional parameters:\n",
    "  - `mean`: The mean (average) of the Gaussian distribution representing the machine's reward. Default value is 0.\n",
    "  - `stdev`: The standard deviation of the Gaussian distribution, which controls the spread or variability of the rewards. Default value is 1.\n",
    "\n",
    "#### Method: `pull_lever()`\n",
    "- The `pull_lever()` method simulates pulling the lever of the bandit machine and returns a reward.\n",
    "- It generates a random reward from the Gaussian distribution with the specified mean and standard deviation using NumPy's `np.random.normal()` function.\n",
    "- The reward is then rounded to one decimal place to provide a realistic output, as slot machines often have discrete reward values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GaussianBandit(object):\n",
    "    def __init__(self, mean=0, stdev=1):\n",
    "        self.mean = mean\n",
    "        self.stdev = stdev\n",
    "\n",
    "    def pull_lever(self):\n",
    "        reward = np.random.normal(self.mean, self.stdev)\n",
    "        return np.round(reward, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slot Machine Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll expand upon the previously defined `GaussianBandit` class and introduce a new class called `GaussianBanditGame`. This new class simulates a game environment in which the player interacts with a set of bandit machines, making choices to pull levers and observe rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructor:\n",
    "- The `GaussianBanditGame` class has a constructor `__init__(self, bandits)`, which takes a list of `GaussianBandit` instances as input. These instances represent the different bandit machines available in the game.\n",
    "- The constructor shuffles the list of bandit machines to randomize their order for the game.\n",
    "\n",
    "#### Method: `play(choice)`\n",
    "- The `play(choice)` method allows the player to pull the lever of a chosen bandit machine and obtain a reward.\n",
    "- The `choice` parameter indicates the index of the selected bandit machine (1-based index).\n",
    "- The method returns the reward obtained from the selected machine.\n",
    "\n",
    "#### Method: `user_play()`\n",
    "- The `user_play()` method initiates the game and allows the user to interact with the bandit machines.\n",
    "- It starts the game loop where the user can make choices to pull levers and observe rewards.\n",
    "- After each round, the method displays the obtained reward and the player's average reward so far.\n",
    "- The game continues until the user enters an invalid choice (not within the valid range) or inputs 0 to end the game.\n",
    "- Upon ending the game, it displays the total reward earned and the average reward per round.\n",
    "\n",
    "#### Method: reset_game()\n",
    "- The `reset_game()` method resets the game's internal state, clearing previous rewards and statistics, and prepares the game for a new session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBanditGame(object):\n",
    "    def __init__(self, bandits):\n",
    "        self.bandits = bandits\n",
    "        np.random.shuffle(self.bandits)\n",
    "        self.reset_game()\n",
    "\n",
    "    def play(self, choice):\n",
    "        reward = self.bandits[choice - 1].pull_lever()\n",
    "        self.rewards.append(reward)\n",
    "        self.total_reward += reward\n",
    "        self.n_played += 1\n",
    "        return reward\n",
    "    \n",
    "    def user_play(self):\n",
    "        self.reset_game()\n",
    "        print(\"Game started. Enter 0 as input to end the game\")\n",
    "        while True:\n",
    "            print(f\"\\n -- Round {self.n_played} -- \")\n",
    "            choice = int(input(f\"Choose a machine from 1 to {len(self.bandits)}: \"))\n",
    "            if choice in range(1, len(self.bandits) + 1):\n",
    "                reward = self.play(choice)\n",
    "                print(f\"Machine {choice} gave a reward of {reward}\")\n",
    "                avg_rew = self.total_reward / self.n_played\n",
    "                print(f\"Your average reward so far is {avg_rew}\")\n",
    "            else:\n",
    "                break\n",
    "        print(\"Game has ended.\")\n",
    "        if self.n_played > 0:\n",
    "            print(f\"Total reward is {self.total_reward} after {self.n_played} round(s).\")\n",
    "            avg_rew = self.total_reward / self.n_played\n",
    "            print(f\"Average reward is {avg_rew}.\")\n",
    "\n",
    "    def reset_game(self):\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "        self.n_played = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "slotA = GaussianBandit(5, 3)\n",
    "slotB = GaussianBandit(6, 2)\n",
    "slotC = GaussianBandit(1, 5)\n",
    "game = GaussianBanditGame([slotA, slotB, slotC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game started. Enter 0 as input to end the game\n",
      "\n",
      " -- Round 0 -- \n",
      "Machine 2 gave a reward of 5.7\n",
      "Your average reward so far is 5.7\n",
      "\n",
      " -- Round 1 -- \n",
      "Machine 2 gave a reward of 7.4\n",
      "Your average reward so far is 6.550000000000001\n",
      "\n",
      " -- Round 2 -- \n",
      "Machine 2 gave a reward of 2.7\n",
      "Your average reward so far is 5.266666666666667\n",
      "\n",
      " -- Round 3 -- \n",
      "Machine 2 gave a reward of 1.8\n",
      "Your average reward so far is 4.4\n",
      "\n",
      " -- Round 4 -- \n",
      "Machine 2 gave a reward of 2.7\n",
      "Your average reward so far is 4.0600000000000005\n",
      "\n",
      " -- Round 5 -- \n",
      "Game has ended.\n",
      "Total reward is 20.3 after 5 round(s).\n",
      "Average reward is 4.0600000000000005.\n"
     ]
    }
   ],
   "source": [
    "game.user_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multi-Armed Bandit problem involves the challenge of choosing between multiple slot machines (bandit machines) that provide random rewards, without knowing the true reward distributions. The player's objective is to maximize their total reward over multiple rounds of play by balancing exploration (trying different machines to learn about rewards) and exploitation (choosing the machine that seems best based on current information).\n",
    "\n",
    "In this example, we observe that early rewards may not accurately represent the true average reward of a machine. Thus, continuous exploration is vital to make informed decisions. The game highlights the importance of data-driven strategies and continuous learning to adapt to uncertainty and stochasticity in the bandit machines.\n",
    "\n",
    "Ultimately, the Multi-Armed Bandit problem serves as a fundamental challenge in decision-making under uncertainty, with applications in various real-world scenarios, including clinical trials, recommendation systems, online advertising, and resource allocation in machine learning. Efficiently addressing this problem requires finding the right balance between exploration and exploitation to maximize rewards and resources over time.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- `Unknown Reward Distributions:` The player does not have prior knowledge of the true reward distributions associated with each machine. As a result, the player needs to explore the machines to learn their reward characteristics.\n",
    "- `Stochastic Rewards:` When the player pulls the lever of a machine, they receive a reward sampled from the machine's reward distribution. These rewards are stochastic, meaning they are subject to randomness and can vary from round to round.\n",
    "- `Exploration vs. Exploitation:` The player faces a trade-off between exploration and exploitation. Exploration involves trying out different machines to gather information about their rewards. Exploitation involves choosing the machine that appears to have the highest reward based on the current knowledge.\n",
    "- `Balancing Strategies:` Striking the right balance between exploration and exploitation is crucial for optimizing the total reward over time. Early in the game, the player may explore more to gather data on the machines' rewards, and as more information is acquired, the player may shift toward exploiting the best machine more frequently.\n",
    "- `Continuous Learning:` The player needs to continuously adapt their strategy as they gain more data and update their knowledge about the machines' reward distributions.\n",
    "- `Convergence to Best Machine:` Over time, with enough exploration and exploitation, the player aims to converge on the machine that provides the highest average reward, maximizing their overall earnings.\n",
    "\n",
    "The Multi-Armed Bandit problem has applications in various real-world scenarios, such as clinical trials, recommendation systems, online advertising, and resource allocation in machine learning. Efficiently addressing this problem is essential in scenarios where making the best choice from uncertain options can lead to significant gains in rewards, resources, or outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env-pricai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
